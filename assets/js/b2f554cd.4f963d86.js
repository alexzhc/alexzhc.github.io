"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"live","metadata":{"permalink":"/blog/live","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-06-20-livestream.md","source":"@site/blog/2022-06-20-livestream.md","title":"Livestream Highlights: Cloud Native Storage","description":"Livestream Highlights","date":"2022-06-20T00:00:00.000Z","formattedDate":"June 20, 2022","tags":[{"label":"livestream","permalink":"/blog/tags/livestream"}],"readingTime":8.965,"truncated":false,"authors":[{"name":"AdaLiu","title":"Technical Writer","url":"https://github.com/AdaLiuyl","image-url":"https://avatars.githubusercontent.com/u/95339758?v","key":"Ada"},{"name":"Michelle Wu","title":"Technical Writer","url":"https://github.com/Michelle951","imageURL":"https://avatars.githubusercontent.com/u/35519562?v=4","key":"Michelle"}],"frontMatter":{"slug":"live","title":"Livestream Highlights: Cloud Native Storage","authors":["Ada","Michelle"],"tags":["livestream"]},"nextItem":{"title":"HwameiStor Capability, Security, Operation, and Maintenance Test","permalink":"/blog/test"}},"content":"![Livestream Highlights](imgs/livestream-highlights.png)\\r\\n\\r\\nThe rapid development of cloud native industry makes many things possible, but it also poses some challenges for application development and operation. As more and more stateful applications are moved onto container platforms, stateful applications have become a major consumer of storage. As a cornerstone for running applications, storage now becomes a major challenge in the process of containerization. In the era of cloud native, what are the new requirements for storage systems? What are the new opportunities and challenges facing cloud-native storage? What are the popular cloud native storage solutions?\\r\\n\\r\\nOn June 20th, 2022, we had a live discussion with [Pengyun Network](https://www.pengyunnetwork.cn/#/Main) on three topics: cloud-native local storage, cloud-native storage solutions for Kubernetes, and the storage systems needed in the era of cloud native.\\r\\n\\r\\n## HwameiStor\\r\\n\\r\\nAs an infrastructure for containerization in cloud native stack, cloud-native storage exposes underlying storage services to containers and micro-services, and collects storage resources from different media. It can enable stateful workloads to run in containers by providing persistent volumes. CNCF\'s definition for cloud-native storage has three key points. First, it runs on Kubernetes as a container. Second, it uses Kubernetes object classes, mainly custom resource definition (CRD). Last and the most important, it uses a container storage interface (CSI). HwameiStor is developed based on these three points. it is an end-to-end cloud-native local storage system.\\r\\n\\r\\n![HwameiStor Architecture](imgs/hwameistor-architecture.png)\\r\\n\\r\\nThe bottom layer of HwameiStor is a local disk manager (LDM). LDM automatically collects resources from various storage media (such as HDD, SSD and NVMe disks) to form a local storage resource pool for unified automatic management. After HwameiStor is deployed, it can allocate resources on various storage media to different pools. Above the resource pool, it uses logical volume manager (LVM) for management, It also uses the CSI architecture to provide distributed local data volume service, so as to provide data persistence capabilities for cloud-native stateful applications or components.\\r\\n\\r\\nHwameiStor is a storage system designed for cloud native. It supports high availability, automation, rapid deployment, and high performance with low cost, making it a good alternative to expensive traditional storage area network (SAN). It has three core components:\\r\\n\\r\\n- Local Disk Management (LDM), which uses CRD to define and manage local data disks. LDM can explicitly get the attributes, size and other information of local disks in Kubernetes.\\r\\n\\r\\n- Local Storage (LS), which uses LVM to allocate logical volumes (LV) to persistent volumes (PV) after LDM is implemented.\\r\\n\\r\\n- Scheduler, which schedules containers to nodes with local data.\\r\\n\\r\\nThe core of HwameiStor lies in the definition and implementation of CRDs. On top of  PersistentVolume (PV) and PersistentVolumeClaim (PVC) object classes in Kubernetes, HwameiStor defines a new object class that can associate PV/PVC in Kubernetes with local data disks.\\r\\n\\r\\nHwameiStor has four features\uff1a\\r\\n\\r\\n- Automatic operation and management: it can automatically discover, identify, manage, and allocate disks, and schedule applications and data according to affinity. It can also automatically monitor disk status and provide timely warning.\\r\\n\\r\\n- Highly available data support: HwameiStor uses cross-node replicas to synchronize data and achieve high availability. When a node goes wrong, it will automatically schedule applications to highly available data nodes to ensure application continuity.\\r\\n\\r\\n- Rich types of data volume: HwameiStor aggregates HDD, SSD, and NVMe disks to provide data services with low latency and high throughput.\\r\\n\\r\\n- Flexible and dynamic linear expansion\uff1aHwameiStor can dynamically expand according to cluster size to meet application\'s requirements for data persistence.\\r\\n\\r\\nHwameiStor is recommended in the following scenarios:\\r\\n\\r\\n- Adapt to middlewares with a highly available architecture\\r\\n  \\r\\n  Some middlewares, like Kafka, ElasticSearch and Redis, have a highly available architecture and a high requirement for IO data access. The LVM-based single-replica local data volumes provided by HwameiStor are suitable for such kind of applications.\\r\\n\\r\\n- Provide highly available data volumes for applications\\r\\n\\r\\n  MySQL and other OLTP databases require their underlying storage systems to provide highly available data storage for quick recovery. Meanwhile, data access should also have a high performance. The highly available dual-replica data volumes with can meet these requirements.\\r\\n\\r\\n- Provide automatic operation and maintenance for traditional storage software\\r\\n\\r\\n  MinIO, Ceph and other similar storage software need to use the disks on Kubernetes nodes. HwameiStor\'s single-replica local volumes can fast respond to the business system\'s needs for deployment, expansion, and migration, thus realizing automatic operation and maintenance based on Kubernetes. This kind of local volumes can be automatically used by PVC/PV through CSI drives.\\r\\n\\r\\n## ZettaStor HASP \\r\\n\\r\\nAccording to the CNCF 2020 Annual Report, stateful applications account for 55% of all container applications, making it a mainstream. About 29% stateful applications listed storage as the main challenge for adopting container technology. A storage problem facing cloud native scenarios is that it is difficult to balance performance and availability. Using a single type of storage cannot meet all needs. Therefore, in the actual implementation of stateful applications, data storage technology is the key.\\r\\n\\r\\n![Storage Solution Comparison](imgs/compare-storage.png)\\r\\n\\r\\nZettaStor HASP is a cloud-native data aggregation & storage platform. It is also a user-state file system of high performance that supports redundancy protection based on multiple replicas across heterogeneous storage systems. Data replicas can flow between different types of storage systems, which is a big competitiveness over traditional distributed storage systems. In addition, it also supports unified and flexible orchestration of storage resources, and can be tightly integrated with container platforms.\\r\\n\\r\\nFor example:\\r\\n\\r\\nZettaStor HASP has a higher data access performance and can realize dynamic allocation and refined management of storage resources. This is desirable for distributed applications that have data redundancy mechanisms themselves and also require high performance, such as Kafka, Redis, MongoDB, and HDFS.\\r\\n\\r\\nZettaStor HASP can achieve data high availability based on cross-node redundancy protection, and meanwhile ensure high performance of local storage. This is suitable for applications that have no data redundancy mechanisms and must rely on external storage, such as MySQL, PostgreSQL.\\r\\n\\r\\n- Critical business should prevent the risk of simultaneous failure of two nodes. ZettaStor HASP\'s replica redundancy protection across local and external storage systems can ensure smooth running of critical business.\\r\\n\\r\\n![ZettaStor HASP Architecture](imgs/zettastor.png)\\r\\n\\r\\nZettaStor HASP has a three-layer architecture. On the top is a high-performance distributed file system, which is also the core of HASP. This independently-developed file system is fully compatible with POSIX standards and supports zero data copy between user-mode and kernel-mode, giving a full play to the performance of high-speed media, such as NVMe SSD/SCM/PM. In the middle is the data service layer, which can provide services across local storage systems on different nodes, across local storage and external storage systems, and across heterogeneous external storage systems. It can be customized into single replica, multiple replicas, strong consistency, and weak consistency solutions. At the bottom is a storage management layer, which is responsible for interacting with different storage devices. It can break device barriers and connect data islands with unified data format, helping data and business get rid of device limitations. In addition, it can also schedule storage devices through CSI.\\r\\n\\r\\nZettaStor HASP is a distributed storage platform closely integrated with container platforms. It supports hyper-converged deployment and CSI integration. It has node affinity, and can sense the running status of user\'s pods, making storage behavior more adaptable.\\r\\n\\r\\n## Round Table Discussion\\r\\n\\r\\nQ1: What is cloud-native storage\\r\\n\\r\\n[alexzhc](https://github.com/alexzhc): In a narrow sense, cloud-native storage need meet three standards. First, it should meet CSI specification and connect well with CS. Second, it should be deployed on Kubernetes as a container. Third, information in the storage system should create a new object class through CRDs and eventually be stored in Kubernetes.\\r\\n\\r\\n[fengqinah](https://github.com/fengqinah): Cloud-native storage has multiple features and can meet various demands. In addition to providing a unified storage platform that can offer different storage features for different applications, it should also connect CSI and establish a bridge between storage systems and Kubernetes for communication.\\r\\n\\r\\n[niulechuan](https://github.com/niulechuan)\uff1aThe most common cloud-native storage solutions are based on cloud storage or distributed storage. Meanwhile, some service providers are also trying to extend the special capabilities of traditional storage.\\r\\n\\r\\nQ2: How should cloud-native storage support cloud-native applications\\r\\n\\r\\n[fengqinah](https://github.com/fengqinah): There are mainly two points. First, cloud-native storage should support features of cloud-native applications, because these features decide the application\'s requirements for storage. Second, it should meet CSI specifications to support special cloud native requirements.\\r\\n\\r\\n[niulechuan](https://github.com/niulechuan): From a performance perspective, cloud-native storage needs to meet all requirements of the CSI architecture, so that it can fit diverse cloud-native scenarios. In order to provide good underlying support and response guarantee for cloud-native applications, cloud-native storage needs efficient operation and maintenance. In real cases, cost, portability, and technical support should also be taken into consideration when designing a cloud-native storage solution. Like cloud-native applications, cloud-native storage also requires \\"elasticity\\". It should have elastic computing capabilities, scalable data, conversions between cold and hot data. In addition, cloud-native storage should also have an open ecosystem.\\r\\n\\r\\nQ3: What are the similarities and differences between cloud-native storage and traditional storage? How about their advantages and disadvantages\\r\\n\\r\\n[alexzhc](https://github.com/alexzhc)\uff1aCloud-native storage is aggregated with Kubernetes after being deployed, while traditional storage is often separated from Kubernetes after being deployed. Cloud-native storage runs on Kubernetes, making it convenient to develop micro-services. If using traditional storage, developers might need to extend the storage API. However, the aggregated form may also, to a certain extent, cause the problem of Kubernetes easily spill over to storage, bringing difficulties to operation and maintenance. Besides, cloud-native storage also has problems about network sharing and disk load.\\r\\n\\r\\n[fengqinah](https://github.com/fengqinah)\uff1aIn external storage, storage nodes and computing nodes have a weak mutual impact. Back-end storage is generally distributed, with relatively high security and availability. However, in cloud native scenarios, only using external storage has certain disadvantages. It will increase network consumption, generate additional cost, and lack linkage with Kubernetes.\\r\\n\\r\\nQ4: How should cloud-native storage empower traditional storage\\r\\n\\r\\n[niulechuan](https://github.com/niulechuan)\uff1aThis is a very urgent need. Kubernetes-native capabilities, such as deletion, creation, and expansion, are mainly implemented through CRDs. The community plays an active role in this process. At the same time, some capabilities of traditional storage have not yet been realized in cloud-native storage, such as cron jobs and observability. How to make cloud-native storage better empower traditional storage on platforms, give full play to their advantages, and further advance cloud native storage? This is a long way to go, and our team is working on this.\\r\\n\\r\\n[alexzhc](https://github.com/alexzhc): To add one more thing. To use a common way to connect Kubernetes and traditional storage, you should aggregate CSI drivers. However, although CSI defines some storage operation flows, it is an interface after all. Therefore, we should consider whether the CSI community should use CRDs to define some features of traditional storage, and whether service providers can define some high-level and special flows by CRDs? We should try to make Kubernetes more applicable in the storage field."},{"id":"test","metadata":{"permalink":"/blog/test","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-06-06-tidb-test.md","source":"@site/blog/2022-06-06-tidb-test.md","title":"HwameiStor Capability, Security, Operation, and Maintenance Test","description":"This test environment is Kubernetes 1.22. We deployed TiDB 1.3 after the HwameiStor local storage is attached. Then, we performed the basic SQL capability test, system security test, and operation and maintenance management test.","date":"2022-06-06T00:00:00.000Z","formattedDate":"June 6, 2022","tags":[{"label":"Test","permalink":"/blog/tags/test"}],"readingTime":14.335,"truncated":false,"authors":[{"name":"zhaosimon","title":"Developer","url":"https://github.com/zhaosimon","imageURL":"https://avatars.githubusercontent.com/u/36882380?v=4","key":"Simon"},{"name":"Michael Yao","title":"Technical Writer","url":"https://github.com/windsonsea","imageURL":"https://github.com/windsonsea.png","key":"Michael"}],"frontMatter":{"slug":"test","title":"HwameiStor Capability, Security, Operation, and Maintenance Test","authors":["Simon","Michael"],"tags":["Test"]},"prevItem":{"title":"Livestream Highlights: Cloud Native Storage","permalink":"/blog/live"},"nextItem":{"title":"LV and LVReplica","permalink":"/blog/3"}},"content":"This test environment is Kubernetes 1.22. We deployed TiDB 1.3 after the HwameiStor local storage is attached. Then, we performed the basic SQL capability test, system security test, and operation and maintenance management test.\\r\\n\\r\\nAll the tests passed successfully, it is acknowledged that HwameiStor can support distributed database application scenarios such as TiDB with high availability, strong consistency requirements, and large data scale.\\r\\n\\r\\n## Introduction to TiDB\\r\\n\\r\\nTiDB is a distributed database product that supports OLTP (Online Transactional Processing), OLAP (Online Analytical Processing), and HTAP (Hybrid Transactional and Analytical Processing) services, compatible with key features such as MySQL 5.7 protocol and MySQL ecosystem. The goal of TiDB is to provide users with one-stop OLTP, OLAP, and HTAP solutions, which are suitable for various application scenarios such as high availability, strict requirements for strong consistency, and large data scale.\\r\\n\\r\\n### TiDB architecture\\r\\n\\r\\nThe TiDB distributed database splits the overall architecture into multiple modules that can communicate with each other. The architecture diagram is as follows:\\r\\n\\r\\n![TiDB architecture](imgs/architecture.png)\\r\\n\\r\\n- **TiDB Server**\\r\\n  \\r\\n  The SQL layer exposes the connection endpoints of the MySQL protocol to the outside world, and is responsible for accepting connections from clients, performing SQL parsing and optimization and finally generating a distributed execution plan. The TiDB layer itself is stateless. In practice, you can start several TiDB instances. A unified access address is provided externally through load-balance components (such as LVS, HAProxy, or F5), and client connections can be evenly distributed on to these TiDB instances. The TiDB server itself does not store data, but only parses SQL and forwards the actual data read request to the underlying storage node, TiKV (or TiFlash).\\r\\n\\r\\n- **PD (Placement Driver) Server**\\r\\n  \\r\\n  The metadata management module across a TiDB cluster is responsible for storing the real-time data distribution of each TiKV node and the overall topology of the cluster, providing the TiDB Dashboard management and control interface, and assigning transaction IDs to distributed transactions. Placement Driver (PD) not only stores metadata, but also issues data scheduling commands to specific TiKV nodes based on the real-time data distribution status reported by TiKV nodes, which can be said to be the \\"brain\\" of the entire cluster. In addition, the PD itself is also composed of at least 3 nodes and has high availability capabilities. It is recommended to deploy an odd number of PD nodes.\\r\\n\\r\\n- **Storage nodes**\\r\\n\\t\\r\\n\\t- TiKV Server: In charge of storing data. From the outside, TiKV is a distributed Key-Value storage engine that provides transactions. The basic unit for storing data is Region. Each Region is responsible for storing the data of a Key Range (the block between left-closed and right-open from StartKey to EndKey). Each TiKV node is responsible for multiple Regions. TiKV API provides native support for distributed transactions at the KV key-value pair level, and provides the levels of Snapshot Isolation (SI) by default, which is also the core of TiDB\'s support for distributed transactions at the SQL level. After the SQL layer of TiDB completes the SQL parsing, it will convert the SQL execution plan into the actual call to the TiKV API. Therefore, the data is stored in TiKV. In addition, the TiKV data will be automatically maintained in multiple replicas (the default is three replicas), which naturally supports high availability and automatic failover.\\r\\n\\r\\n\\t- TiFlash is a special storage node. Unlike ordinary TiKV nodes, data is stored in columns in TiFlash, and the main function is to accelerate analysis-based scenarios.\\r\\n\\r\\n### TiDB database storage\\r\\n\\r\\n![TiDB database storage](imgs/storage.png)\\r\\n\\r\\n- **Key-Value Pair**\\r\\n\\r\\n  The choice of TiKV is the Key-Value model that provides an ordered traversal method. Two key points of TiKV data storage are:\\r\\n\\r\\n  - A huge Map (comparable to std::map in C++) that stores Key-Value Pairs.\\r\\n\\r\\n  - The Key-Value pairs in this Map are sorted by the binary order of the Key, that is, you can seek to the position of a certain Key, and then continuously call the Next method to obtain the Key-Value larger than this Key in an ascending order.\\r\\n\\r\\n- **Local storage (Rocks DB)**\\r\\n  \\r\\n  In any persistent storage engine, data must be saved on disk after all, and TiKV is not different. However, TiKV does not choose to write data directly to the disk, but stores the data in RocksDB, and RocksDB is responsible for the specific data storage. The reason is that developing a stand-alone storage engine requires a lot of work, especially to make a high-performance stand-alone engine, which may require various meticulous optimizations. RocksDB is a very good stand-alone KV storage engine open sourced by Facebook. It can meet various requirements of TiKV for single engine. Here we can simply consider that RocksDB is a persistent Key-Value Map on a host.\\r\\n\\r\\n- **Raft protocol**\\r\\n  \\r\\n  TiKV uses the Raft algorithm to ensure that data is not lost and error-free when a single host fails. In short, it is to replicate data to multiple hosts, so that if one host cannot provide services, replicas on other hosts can still provide services. This data replication scheme is reliable and efficient, and can deal with replica failures.\\r\\n\\r\\n- **Region**\\r\\n  \\r\\n  TiKV divides the Range by Key. A certain segment of consecutive Keys are stored on a storage node. Divide the entire Key-Value space into many segments, each segment is a series of consecutive Keys, called a Region. Try to keep the data saved in each Region within a reasonable size. Currently, the default in TiKV is no more than 96 MB. Each Region can be described by a left-closed and right-open block such as [StartKey, EndKey].\\r\\n\\r\\n- **MVCC**\\r\\n  \\r\\n  TiKV implements Multi-Version Concurrency Control (MVCC).\\r\\n\\r\\n- **Distributed ACID transactions**\\r\\n  \\r\\n  TiKV uses the transaction model used by Google in BigTable: Percolator.\\r\\n\\r\\n## Build the test environment\\r\\n\\r\\n### Kubernetes cluster\\r\\n\\r\\nIn this test, we use three VM nodes to deploy the Kubernetes cluster, including one master node and two worker nodes. Kubelete version is 1.22.0.\\r\\n\\r\\n![k8s cluster](imgs/k8scluster.png)\\r\\n\\r\\n### HwameiStor local storage\\r\\n\\r\\n1. Deploy the HwameiStor local storage in the Kubernetes cluster\\r\\n\\r\\n   ![HwameiStor local storage](imgs/hwameistor.png)\\r\\n\\r\\n2. Configure a 100G local disk, sdb, for HwameiStor on two worker nodes respectively\\r\\n\\r\\n   ![sdb1](imgs/sdb1.png)\\r\\n\\r\\n   ![sdb2](imgs/sdb2.png)\\r\\n\\r\\n3. Create StorageClass\\r\\n\\r\\n   ![create StorageClass](imgs/storageclass.png)\\r\\n\\r\\n### Deploy TiDB on Kubernetes\\r\\n\\r\\nTiDB can be deployed on Kubernetes using TiDB Operator. TiDB Operator is an automatic operation and maintenance system for TiDB clusters on Kubernetes. It provides full lifecycle management of TiDB including deployment, upgrade, scaling, backup and recovery, and configuration changes. With TiDB Operator, TiDB can run seamlessly on public cloud or privately deployed Kubernetes clusters.\\r\\n\\r\\nThe compatibility between TiDB and TiDB Operator versions is as follows:\\r\\n\\r\\n| TiDB  version         | Applicable versions of TiDB Operator |\\r\\n| ------------------ | ------------------------- |\\r\\n| dev                | dev                       |\\r\\n| TiDB  >= 5.4       | 1.3                       |\\r\\n| 5.1  <= TiDB < 5.4 | 1.3 (recommended), 1.2         |\\r\\n| 3.0  <= TiDB < 5.1 | 1.3 (recommended), 1.2, 1.1    |\\r\\n| 2.1  <= TiDB < 3.0 | 1.0 (maintenance stopped)      |\\r\\n\\r\\n#### Deploy TiDB Operator\\r\\n\\r\\n1. Install TiDB CRDs\\r\\n\\r\\n   ```bash\\r\\n   kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/crd.yaml\\r\\n   ```\\r\\n\\r\\n2. Install TiDB Operator\\r\\n\\r\\n   ```bash\\r\\n   helm repo add pingcap https://charts.pingcap.org/ \\r\\n   kubectl create namespace tidb-admin \\r\\n   helm install --namespace tidb-admin tidb-operator pingcap/tidb-operator --version v1.3.2 \\\\\\r\\n   --set operatorImage=registry.cn-beijing.aliyuncs.com/tidb/tidb-operator:v1.3.2 \\\\\\r\\n   --set tidbBackupManagerImage=registry.cn-beijing.aliyuncs.com/tidb/tidb-backup-manager:v1.3.2 \\\\\\r\\n   --set scheduler.kubeSchedulerImageName=registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler\\r\\n   ```\\r\\n\\r\\n3. Check TiDB Operator components\\r\\n\\r\\n   ![check TiDB Operator components](imgs/check.png)\\r\\n\\r\\n#### Deploy the TiDB cluster\\r\\n\\r\\n```bash\\r\\nkubectl create namespace tidb-cluster && \\\\\\r\\nkubectl -n tidb-cluster apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/examples/basic/tidb-cluster.yaml \\r\\nkubectl -n tidb-cluster apply -f https://raw.githubusercontent.com /pingcap/tidb-operator/master/examples/basic/tidb-monitor.yaml\\r\\n```\\r\\n\\r\\n![deploy TiDB cluster](imgs/deploytidb.png)\\r\\n\\r\\n#### Connect the TiDB cluster\\r\\n\\r\\n```bash\\r\\nyum -y install mysql-client\\r\\n```\\r\\n\\r\\n![connect tidb](imgs/connecttidb.png)\\r\\n\\r\\n```bash\\r\\nkubectl port-forward -n tidb-cluster svc/basic-tidb 4000 > pf4000.out & \\r\\n```\\r\\n\\r\\n![connect TiDB cluster](imgs/connect1.png)\\r\\n\\r\\n![connect tidb cluster](imgs/connect2.png)\\r\\n\\r\\n![connect TiDB cluster](imgs/connect3.png)\\r\\n\\r\\n#### Check and verify the TiDB cluster status\\r\\n\\r\\n1. Create the Hello_world table\\r\\n\\r\\n   ```sql\\r\\n   create table hello_world (id int unsigned not null auto_increment primary key, v varchar(32)); \\r\\n   ```\\r\\n\\r\\n   ![create Hello_world table](imgs/helloworld.png)\\r\\n\\r\\n2. Check the TiDB version\\r\\n\\r\\n   ```sql\\r\\n   select tidb_version()\\\\G;\\r\\n   ```\\r\\n\\r\\n   ![check version](imgs/checkversion.png)\\r\\n\\r\\n3. Check the Tikv storage status\\r\\n\\r\\n   ```sql\\r\\n   select * from information_schema.tikv_store_status\\\\G;\\r\\n   ```\\r\\n\\r\\n   ![check storage](imgs/checkstorage.png)\\r\\n\\r\\n#### Configure the HwameiStor storage\\r\\n\\r\\nCreate a PVC for tidb-tikv and tidb-pd from `storageClass local-storage-hdd-lvm`:\\r\\n\\r\\n![HwameiStor storage config](imgs/pvc.png)\\r\\n\\r\\n![pvc1](imgs/pvc1.png)\\r\\n\\r\\n![pvc2](imgs/pvc2.png)\\r\\n\\r\\n```bash\\r\\nkubectl get po basic-tikv-0 -oyaml\\r\\n```\\r\\n\\r\\n![mountpvc](imgs/mountpvc.png)\\r\\n\\r\\n```bash\\r\\nkubectl get po basic-pd-0 -oyaml\\r\\n```\\r\\n\\r\\n![mountpvc1](imgs/mountpvc1.png)\\r\\n\\r\\n## Test procedure\\r\\n\\r\\n### Basic SQL capability test\\r\\n\\r\\nAfter the database cluster is deployed, we performed the following tests about basic capabilities. All are successfully passed.\\r\\n\\r\\n#### Distributed transaction\\r\\n\\r\\nTest purpose: In the case of multiple isolation levels, check if the completeness constraints of distributed data operations are supported, such as atomicity, consistency, isolation, and durability (ACID)\\r\\n\\r\\nTest steps:\\r\\n\\r\\n1. Create the database: testdb\\r\\n\\r\\n2. Create the table `t_test ( id int AUTO_INCREMENT, name varchar(32), PRIMARY KEY (id) )`\\r\\n\\r\\n3. Run a test script\\r\\n\\r\\nTest result: The completeness constraints of distributed data operations are supported, such as atomicity, consistency, isolation, and durability (ACID), in the case of multiple isolation levels\\r\\n\\r\\n#### Object isolation\\r\\n\\r\\nTest purpose: Check if the object isolation can be implemented by using different schemas\\r\\n\\r\\nTest script:\\r\\n\\r\\n```sql\\r\\ncreate database if not exists testdb;\\r\\nuse testdb\\r\\ncreate table if not exists t_test\\r\\n( id                   bigint,\\r\\n  name                 varchar(200),\\r\\n  sale_time            datetime default current_timestamp,\\r\\n  constraint pk_t_test primary key (id)\\r\\n);\\r\\ninsert into t_test(id,name) values (1,\'a\'),(2,\'b\'),(3,\'c\');\\r\\ncreate user \'readonly\'@\'%\' identified by \\"readonly\\";\\r\\ngrant select on testdb.* to readonly@\'%\';\\r\\nselect * from testdb.t_test;\\r\\nupdate testdb.t_test set name=\'aaa\';\\r\\ncreate user \'otheruser\'@\'%\' identified by \\"otheruser\\";\\r\\n```\\r\\n\\r\\nTest result: Supported to create different schemas to implement the object isolation\\r\\n\\r\\n#### Table operation support\\r\\n\\r\\nTest purpose: Check if you can create, delete, and modifiy table data, DML, columns, partition table\\r\\n\\r\\nTest steps: Run the test scripts step by step after connecting the database\\r\\n\\r\\nTest script:\\r\\n\\r\\n```sql\\r\\n# Create and delete table\\r\\ndrop table if exists t_test;\\r\\ncreate table if not exists t_test\\r\\n( id                   bigint default \'0\',\\r\\n  name                 varchar(200) default \'\' ,\\r\\n  sale_time            datetime default current_timestamp,\\r\\n  constraint pk_t_test primary key (id)\\r\\n);\\r\\n# Delete and modify\\r\\ninsert into t_test(id,name) values (1,\'a\'),(2,\'b\'),(3,\'c\'),(4,\'d\'),(5,\'e\');\\r\\nupdate t_test set name=\'aaa\' where id=1;\\r\\nupdate t_test set name=\'bbb\' where id=2;\\r\\ndelete from t_dml where id=5;\\r\\n# Modify, add, delete columns\\r\\nalter table t_test modify column name varchar(250);\\r\\nalter table t_test add column col varchar(255);\\r\\ninsert into t_test(id,name,col) values(10,\'test\',\'new_col\');     \\r\\nalter table t_test add column colwithdefault varchar(255) default \'aaaa\';\\r\\ninsert into t_test(id,name) values(20,\'testdefault\');\\r\\ninsert into t_test(id,name,colwithdefault ) values(10,\'test\',\'non-default \');     \\r\\nalter table t_test drop column colwithdefault;\\r\\n# Type of partition table (only listed part of scripts)\\r\\nCREATE TABLE employees (\\r\\n    id INT NOT NULL,\\r\\nfname VARCHAR(30),\\r\\nlname VARCHAR(30),\\r\\n    hired DATE NOT NULL DEFAULT \'1970-01-01\',\\r\\n    separated DATE NOT NULL DEFAULT \'9999-12-31\',\\r\\njob_code INT NOT NULL,\\r\\nstore_id INT NOT NULL\\r\\n)\\r\\n```\\r\\n\\r\\nTest result: Supported to create, delete, and modifiy table data, DML, columns, partition table\\r\\n\\r\\n#### Index support\\r\\n\\r\\nTest purpose: Verify different indexes (unique, clustered, partitioned, Bidirectional indexes, Expression-based indexes, hash indexes, etc.) and index rebuild operations.\\r\\n\\r\\nTest script:\\r\\n\\r\\n```bash\\r\\nalter table t_test add unique index udx_t_test (name);\\r\\n# The default is clustered index of primary key\\r\\nADMIN CHECK TABLE t_test;\\r\\ncreate index time_idx on t_test(sale_time);\\r\\nalter table t_test drop index time_idx;\\r\\nadmin show ddl jobs;\\r\\nadmin show ddl job queries 156;\\r\\ncreate index time_idx on t_test(sale_time);\\r\\n```\\r\\n\\r\\nTest result: Supported to create, delete, combine, and list indexes and supported for unique index\\r\\n\\r\\n#### Statements\\r\\n\\r\\nTest purpose: Check if the statements in distributed databases are supported such as `if`, `case when`, `for loop`, `while loop`, `loop exit when` (up to 5 kinds)\\r\\n\\r\\nTest script:\\r\\n\\r\\n```sql\\r\\nSELECT CASE id WHEN 1 THEN \'first\' WHEN 2 THEN \'second\' ELSE \'OTHERS\' END AS id_new  FROM t_test;\\r\\nSELECT IF(id>2,\'int2+\',\'int2-\') from t_test;\\r\\n\\r\\n```\\r\\n\\r\\nTest result: supported for statements such as `if`, `case when`, `for loop`, `while loop`, and `loop exit when` (up to 5 kinds)\\r\\n\\r\\n#### Parsing execution plan\\r\\n\\r\\nTest purpose: Check if execution plan parsing is supported for distributed databases\\r\\n\\r\\nTest script:\\r\\n\\r\\n```sql\\r\\nexplain analyze select * from t_test where id NOT IN (1,2,4);\\r\\nexplain analyze select * from t_test a where EXISTS (select * from t_test b where a.id=b.id and b.id<3);\\r\\nexplain analyze SELECT IF(id>2,\'int2+\',\'int2-\') from t_test;\\r\\n```\\r\\n\\r\\nTest result: the execution plan is supported to parse\\r\\n\\r\\n#### Binding execution plan\\r\\n\\r\\nTest purpose: Verify the feature of binding execution plan for distributed databases\\r\\n\\r\\nTest steps:\\r\\n\\r\\n1. View the current execution plan of sql statements\\r\\n\\r\\n2. Use the binding feature\\r\\n\\r\\n3. View the execution plan after the sql statement is binded\\r\\n\\r\\n4. Delete the binding\\r\\n\\r\\nTest script:\\r\\n\\r\\n```sql\\r\\nexplain select * from employees3 a join employees4 b on a.id = b.id where a.lname=\'Johnson\';\\r\\nexplain select /*+ hash_join(a,b) */ * from employees3 a join employees4 b on a.id = b.id where a.lname=\'Johnson\';\\r\\n```\\r\\n\\r\\nTest result: It may not be hash_join when hint is not used, and it must be hash_join after hint is used.\\r\\n\\r\\n#### Common functions\\r\\n\\r\\nTest purpose: Verify standard functions of distributed databases\\r\\n\\r\\nTest result: Standard database functions are supported\\r\\n\\r\\n#### Explicit/implicit transactions\\r\\n\\r\\nTest purpose: Verify the transaction support of distributed databases\\r\\n\\r\\nTest result: Explict and implicit transactions are supported\\r\\n\\r\\n#### Character set\\r\\n\\r\\nTest purpose: Verify the data types supported by distributed database\\r\\n\\r\\nTest result: Only the UTF-8 mb4 character set is supported now\\r\\n\\r\\n#### Lock support\\r\\n\\r\\nTest purpose: Verify the lock implementation of distributed databases\\r\\n\\r\\nTest result: Described how the lock is implemented, what are blockage conditions in the case of R-R/R-W/W-W, and how the deadlock is handled\\r\\n\\r\\n#### Isolation levels\\r\\n\\r\\nTest purpose: Verify the transactional isolation levels of distributed databases\\r\\n\\r\\nTest result: Supported for si and rc isolation levels (4.0 GA version)\\r\\n\\r\\n#### Distributed complex query\\r\\n\\r\\nTest purpose: Verify the complex query capabilities of distributed databases\\r\\n\\r\\nTest result: Supported for the distributed complex queries and operations such as inter-node joins, and supported for window functions and hierarchical queries\\r\\n\\r\\n### System security test\\r\\n\\r\\nThis section describes system security tests. After the database cluster is deployed, all the following tests are passed.\\r\\n\\r\\n#### Account management and permission test\\r\\n\\r\\nTest purpose: Verify the accout permisson management of distributed databases\\r\\n\\r\\nTest script:\\r\\n\\r\\n```sql\\r\\nselect host,user,authentication_string from mysql.user;\\r\\ncreate user tidb IDENTIFIED by \'tidb\'; \\r\\nselect host,user,authentication_string from mysql.user;\\r\\nset password for tidb =password(\'tidbnew\');\\r\\nselect host,user,authentication_string,Select_priv from mysql.user;\\r\\ngrant select on *.* to tidb;\\r\\nflush privileges ;\\r\\nselect host,user,authentication_string,Select_priv from mysql.user;\\r\\ngrant all privileges on *.* to tidb;\\r\\nflush privileges ;\\r\\nselect * from  mysql.user where user=\'tidb\';\\r\\nrevoke select on *.* from tidb; \\r\\nflush privileges ;\\r\\nrevoke all privileges on *.* from tidb;\\r\\nflush privileges ;\\r\\ngrant select(id) on test.TEST_HOTSPOT to tidb;\\r\\ndrop user tidb;\\r\\n```\\r\\n\\r\\nTest results:\\r\\n\\r\\n- Supported for creating, modifying, and deleting accounts, and configuring passwords, and supported for the separation of security, audit, and data management\\r\\n\\r\\n- Based on different accounts, various permission control for database includes: instance, library, table, and column\\r\\n\\r\\n#### Access control\\r\\n\\r\\nTest purpose: Verify the permission access control of distributed databases, and control the database data by granting basic CRUD (create, read, update, and delete) permissions\\r\\n\\r\\nTest script:\\r\\n\\r\\n```sql\\r\\nmysql -u root -h 172.17.49.222 -P 4000\\r\\ndrop user tidb;\\r\\ndrop user tidb1;\\r\\ncreate user tidb IDENTIFIED by \'tidb\'; \\r\\ngrant select on tidb.* to tidb;\\r\\ngrant insert on tidb.* to tidb;\\r\\ngrant update on tidb.* to tidb;\\r\\ngrant delete on tidb.* to tidb;\\r\\nflush privileges;\\r\\nshow grants for tidb;\\r\\nexit;\\r\\nmysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e \'select * from aa;\'\\r\\nmysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e \'insert into aa values(2);\'\\r\\nmysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e \'update aa set id=3;\'\\r\\nmysql -u tidb -h 172.17.49.222 -ptidb -P 4000 -D tidb -e \'delete from aa where id=3;\'\\r\\n```\\r\\n\\r\\nTest result: Database data is controlled by granting the basic CRUD permissions\\r\\n\\r\\n#### Whitelist\\r\\n\\r\\nTest purpose: Verify the whitelist feature of distributed databases\\r\\n\\r\\nTest script:\\r\\n\\r\\n```sql\\r\\nmysql -u root -h 172.17.49.102 -P 4000\\r\\ndrop user tidb;\\r\\ncreate user tidb@\'127.0.0.1\' IDENTIFIED by \'tidb\'; \\r\\nflush privileges;\\r\\nselect * from mysql.user where user=\'tidb\';\\r\\nmysql -u tidb -h 127.0.0.1 -P 4000 -ptidb\\r\\nmysql -u tidb -h 172.17.49.102 -P 4000 -ptidb\\r\\n```\\r\\n\\r\\nTest result: Supported for the IP whitelist feature and supportred for matching actions with IP segments\\r\\n\\r\\n#### Operation log\\r\\n\\r\\nTest purpose: Verify the monitor capability to distributed databases\\r\\n\\r\\nTest script: `kubectl -ntidb-cluster logs tidb-test-pd-2 --tail 22`\\r\\n\\r\\nTest result: Record key actions or misoperations performed by users through the operation and maintenance management console or API\\r\\n\\r\\n### Operation and maintenance test\\r\\n\\r\\nThis section describes the operation and maintenance test. After the database cluster is deployed, the following operation and maintenance tests are all passed.\\r\\n\\r\\n#### Import and export data\\r\\n\\r\\nTest purpose: Verify the tools support for importing and exporting data of distributed databases\\r\\n\\r\\nTest script:\\r\\n\\r\\n```sql\\r\\nselect * from sbtest1 into outfile \'/sbtest1.csv\';\\r\\nload data local infile \'/sbtest1.csv\' into table test100;\\r\\n```\\r\\n\\r\\nTest result: Supported for importing and exporting table, schema, and database\\r\\n\\r\\n#### Slow log query\\r\\n\\r\\nTest purpose: Get the SQL info by slow query\\r\\n\\r\\nPrerequisite: The SQL execution time shall be longer than the configured threshold for slow query, and the SQL execution is completed\\r\\n\\r\\nTest steps:\\r\\n\\r\\n1. Adjust the slow query threshold to 100 ms\\r\\n\\r\\n2. Run SQL\\r\\n\\r\\n3. View the slow query info from log, system table, or dashboard\\r\\n\\r\\nTest script:\\r\\n\\r\\n```sql\\r\\nshow variables like \'tidb_slow_log_threshold\';\\r\\nset tidb_slow_log_threshold=100;\\r\\nselect query_time, query from information_schema.slow_query where is_internal = false order by query_time desc limit 3;\\r\\n```\\r\\n\\r\\nTest result: Can get the slow query info.\\r\\n\\r\\nFor details about test data, see [TiDB on HwameiStor Deployment and Test Logs](imgs/TiDBonHwameiStor.docx)."},{"id":"3","metadata":{"permalink":"/blog/3","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-05-24-lv-and-lvreplica.md","source":"@site/blog/2022-05-24-lv-and-lvreplica.md","title":"LV and LVReplica","description":"In Kubernetes, when a PVC is created and uses HwameiStor as its local storage, HwameiStor will create two kinds of CR: LocalVolume and LocalVolumeReplica. Why create these two resources for one PV? Continue to read and you will find the answer.","date":"2022-05-24T00:00:00.000Z","formattedDate":"May 24, 2022","tags":[{"label":"hello","permalink":"/blog/tags/hello"},{"label":"Hwameistor","permalink":"/blog/tags/hwameistor"}],"readingTime":4.225,"truncated":false,"authors":[{"name":"Lechuan Niu","title":"Developer","url":"https://github.com/niulechuan","imageURL":"https://avatars.githubusercontent.com/u/81207605?v=4","key":"Niulechuan"},{"name":"Michelle Wu","title":"Technical Writer","url":"https://github.com/Michelle951","imageURL":"https://avatars.githubusercontent.com/u/35519562?v=4","key":"Michelle"}],"frontMatter":{"slug":"3","title":"LV and LVReplica","authors":["Niulechuan","Michelle"],"tags":["hello","Hwameistor"]},"prevItem":{"title":"HwameiStor Capability, Security, Operation, and Maintenance Test","permalink":"/blog/test"},"nextItem":{"title":"Reliable Helper System for HwameiStor Is Online","permalink":"/blog/2"}},"content":"\x3c!--\u5728Kubernetes\u4e2d\uff0c\u5f53\u7528\u6237\u521b\u5efa\u4e00\u4e2aPVC\uff0c\u5e76\u6307\u5b9a\u4f7f\u7528Hwameistor\u4f5c\u4e3a\u5e95\u5c42\u5b58\u50a8\u65f6\uff0cHwameistor\u4f1a\u521b\u5efa\u4e24\u7c7bCR\uff0c\u5373\u672c\u6587\u7684\u4e3b\u89d2`LocalVolume`,`LocalVolumeReplica`. \u90a3\u4e3a\u4ec0\u4e48Hwameistor\u4f1a\u4e3a\u4e00\u4e2aPV\u521b\u5efa\u8fd9\u4e24\u7c7b\u8d44\u6e90\u5462\uff1f\u672c\u6587\u5c06\u4e3a\u60a8\u63ed\u5f00\u8c1c\u56e2\u3002--\x3e\\nIn Kubernetes, when a PVC is created and uses HwameiStor as its local storage, HwameiStor will create two kinds of CR: `LocalVolume` and `LocalVolumeReplica`. Why create these two resources for one PV? Continue to read and you will find the answer.\\n\\n![LV Replicas](images/lv_replicas_en.png)\\n\\n## LocalVolume\\n\\n\x3c!--`LocalVolume`\uff1a\u662fHwameistor\u5b9a\u4e49\u7684CRD\uff0c\u4ee3\u8868Hwameistor\u4e3a\u7528\u6237\u63d0\u4f9b\u7684\u6570\u636e\u5377\uff0c`LocalVolume`\u548cKubernetes\u7684`PersistentVolume`\u662f\u4e00\u4e00\u5bf9\u5e94\u7684\uff0c\u542b\u4e49\u4e5f\u662f\u7c7b\u4f3c\u7684\uff0c\u5747\u4ee3\u8868\u4e00\u4e2a\u6570\u636e\u5377\uff0c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e`LocalVolume`\u4f1a\u8bb0\u5f55Hwameistor\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u800c`PersistentVolume`\u4f1a\u8bb0\u5f55Kubernetes\u5e73\u53f0\u672c\u8eab\u7684\u4fe1\u606f\uff0c\u5e76\u5173\u8054\u5230`LocalVolume`.--\x3e\\n`LocalVolume` is a CRD defined by HwameiStor. It is the volume that HwameiStor provides for users. Each `LocalVolume` corresponds to a `PersistentVolume` of Kubernetes. Both are volumes, but `LocalVolume` stores HwameiStor-related information, while the other records information about Kubernetes itself and links it to `LocalVolume`.\\n\\n\x3c!--\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u7cfb\u7edf\u4e2d`LocalVolume`\u7684\u8be6\u7ec6\u4fe1\u606f\uff1a--\x3e\\nYou can check details of `LocalVolume` with this command:\\n\\n```\\n#  check status of local volume and volume replica\\n$ kubectl get lv # or localvolume\\nNAME                                       POOL                   KIND   REPLICAS   CAPACITY     ACCESSIBILITY   STATE      RESOURCE   PUBLISHED   AGE\\npvc-996b05e8-80f2-4240-ace4-5f5f250310e2   LocalStorage_PoolHDD   LVM    1          1073741824   k8s-node1       Ready   -1                     22m\\n```\\n\\n\x3c!--\u65e2\u7136Hwameistor\u53ef\u4ee5\u901a\u8fc7`LocalVolume`\u8868\u793a\u4e00\u4e2a\u6570\u636e\u5377\uff0c\u4e3a\u4ec0\u4e48\u8fd8\u9700\u8981`LocalVolumeReplica`\u5462\uff1f--\x3e\\nNow that HwameiStor can use `LocalVolume` to provide a volume, why do we still need `LocalVolumeReplica`?\\n\\n## LocalVolumeReplica\\n\\n\x3c!--`LocalVolumeReplica`\uff1a\u4e5f\u662fHwameistor\u5b9a\u4e49\u7684CRD\uff0c\u4f46\u662f\u4e0e`LocalVolume`\u4e0d\u540c\uff0c`LocalVolumeReplica`\u4ee3\u8868\u6570\u636e\u5377\u7684\u526f\u672c\u3002--\x3e\\n`LocalVolumeReplica` is another CRD defined by HwameiStor. It represents a replica of a volume.\\n\\n\x3c!--\u5728Hwameistor\u4e2d\uff0c`LocalVolume`\u4f1a\u6307\u5b9a\u67d0\u4e2a\u5c5e\u4e8e\u5b83\u7684`LocalVolumeReplica`\u4f5c\u4e3a\u5f53\u524d\u6fc0\u6d3b\u7684\u526f\u672c\u3002\u53ef\u4ee5\u770b\u51fa`LocalVolume`\u53ef\u4ee5\u62e5\u6709\u591a\u4e2a`LocalVolumeReplica`\uff0c\u5373\u4e00\u4e2a\u6570\u636e\u5377\u53ef\u4ee5\u6709\u591a\u4e2a\u526f\u672c\u3002\u76ee\u524dHwameistor\u4f1a\u5728\u4f17\u591a\u526f\u672c\u4e2d\u6fc0\u6d3b\u5176\u4e2d\u4e00\u4e2a\uff0c\u88ab\u5e94\u7528\u7a0b\u5e8f\u6302\u8f7d\uff0c\u5176\u4ed6\u4f5c\u4e3a\u70ed\u5907\u526f\u672c\u3002--\x3e\\nIn HwameiStor, `LocalVolume` can specify one of its `LocalVolumeReplica` as the active replica. As a volume, `LocalVolume` can have many `LocalVolumeReplica` as its replicas. The replica in active state will be mounted by applications and others will stand by as high available replicas.\\n\\n\x3c!--\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u7cfb\u7edf\u4e2d`LocalVolumeReplica`\u7684\u8be6\u7ec6\u4fe1\u606f\uff1a--\x3e\\nYou can check details of `LocalVolumeReplica` with this command:\\n\\n```\\n$ kubectl get lvr # or localvolumereplica\\nNAME                                              KIND   CAPACITY     NODE        STATE   SYNCED   DEVICE                                                               AGE\\npvc-996b05e8-80f2-4240-ace4-5f5f250310e2-v5scm9   LVM    1073741824   k8s-node1   Ready   true     /dev/LocalStorage_PoolHDD/pvc-996b05e8-80f2-4240-ace4-5f5f250310e2   80s\\n```\\n\\n\x3c!--\u6709\u4e86\u5377\u526f\u672c\uff08LocalVolumeReplica\uff09\u7684\u6982\u5ff5\u540eHwameistor\u4f5c\u4e3a\u4e00\u6b3e\u672c\u5730\u5b58\u50a8\u7cfb\u7edf\uff0c\u5177\u5907\u4e86\u4e00\u4e9b\u5f88\u6709\u7ade\u4e89\u529b\u7684\u7279\u6027\uff0c\u4f8b\u5982\u6570\u636e\u5377\u7684HA\uff0c\u8fc1\u79fb\uff0c\u70ed\u5907\uff0cKubernetes\u5e94\u7528\u5feb\u901f\u6062\u590d\u7b49\u7b49\u3002--\x3e\\n`LocalVolumeReplica` allows HwameiStor to support features like HA, migration, hot standby of volumes and fast recovery of Kubernetes applications, making it more competitive as a local storage tool.\\n\\n\x3c!--## \u603b\u7ed3--\x3e\\n## Conclusion\\n\\n\x3c!--\u5176\u5b9e`LocalVolume`\u548c`LocalVolumeReplica`\u5728\u5f88\u591a\u5b58\u50a8\u7cfb\u7edf\u4e2d\u90fd\u6709\u5f15\u5165\uff0c\u662f\u4e2a\u5f88\u901a\u7528\u7684\u6982\u5ff5\uff0c\u53ea\u662f\u901a\u8fc7\u8fd9\u4e00\u6982\u5ff5\uff0c\u5b9e\u73b0\u4e86\u5404\u5177\u7279\u8272\u7684\u4ea7\u54c1\uff0c\u5728\u89e3\u51b3\u67d0\u4e2a\u6280\u672f\u96be\u70b9\u7684\u65f6\u5019\u4e5f\u53ef\u80fd\u91c7\u53d6\u4e0d\u540c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u56e0\u6b64\u800c\u9002\u5408\u4e8e\u4e0d\u540c\u7684\u751f\u4ea7\u573a\u666f\u3002--\x3e\\n`LocalVolume` and `LocalVolumeReplica` are common concepts in many storage products, but each product can have its own competitive and unique features based on these two concepts. A technical difficulty can be solved with different solutions, so these concepts are also suitable for different production scenarios.\\n\\n\x3c!--\u968f\u7740Hwameistor\u7684\u8fed\u4ee3\u548c\u6f14\u8fdb\uff0c\u6211\u4eec\u5c06\u4f1a\u63d0\u4f9b\u66f4\u591a\u7684\u80fd\u529b\uff0c\u4ece\u800c\u9002\u914d\u8d8a\u6765\u8d8a\u591a\u7684\u4f7f\u7528\u573a\u666f\u3002\u65e0\u8bba\u60a8\u662f\u7528\u6237\u8fd8\u662f\u5f00\u53d1\u8005\uff0c\u6b22\u8fce\u60a8\u52a0\u5165Hwameistor\u7684\u5927\u5bb6\u5ead\uff01--\x3e\\nWe will provide more capabilities for more scenarios in future releases. Both users and developers are welcome to join us!"},{"id":"2","metadata":{"permalink":"/blog/2","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-05-19-helper-system-post.md","source":"@site/blog/2022-05-19-helper-system-post.md","title":"Reliable Helper System for HwameiStor Is Online","description":"News today: The HwameiStor Reliable Helper System, an automatic and reliable cloud-native local storage maintenance system dedicated to system operation and maintenance, has been launched.","date":"2022-05-19T00:00:00.000Z","formattedDate":"May 19, 2022","tags":[{"label":"hello","permalink":"/blog/tags/hello"},{"label":"Hwameistor","permalink":"/blog/tags/hwameistor"}],"readingTime":1.54,"truncated":false,"authors":[{"name":"Jie Li","title":"Developer","url":"https://github.com/angel0507","imageURL":"https://avatars.githubusercontent.com/u/39614083?v=4","key":"JieLi"},{"name":"Michael Yao","title":"Technical Writer","url":"https://github.com/windsonsea","imageURL":"https://github.com/windsonsea.png","key":"Michael"}],"frontMatter":{"slug":"2","title":"Reliable Helper System for HwameiStor Is Online","authors":["JieLi","Michael"],"tags":["hello","Hwameistor"]},"prevItem":{"title":"LV and LVReplica","permalink":"/blog/3"},"nextItem":{"title":"HwameiStor Comes Online","permalink":"/blog/1"}},"content":"> News today: The HwameiStor Reliable Helper System, an automatic and reliable cloud-native local storage maintenance system dedicated to system operation and maintenance, has been launched.\\n\\nDaoCloud officially opens source of HwameiStor Reliable Helper System, a cloud native, automatic, reliable local storage maintenance system. This system is still in the alpha stage. HwameiStor creates a local storage pool with HDD, SSD, and NVMe disks for a central management. As the underlying data base used by applications, disks often face risks such as natural and unintentional damage. In this case, the Reliable Helper System comes out for disk operation and maintenance. All developers and enthusiasts are welcome to try it out.\\n\\n![System architecture](images/HwameiStor-replace-disk-arch.jpg)\\n\\nIn the cloud native era, application developers can focus on the business logic itself, while the agility, scalability, and reliability required by the application runtime attribute to the infrastructure platform and O\\\\&M team. The HwameiStor Reliable Helper System is a reliable operation and maintenance system that meets the requirements of the cloud-native era. Currently, it supports the feature of one-click disk replacement.\\n\\n## Comprehensively enhance the operation and maintenance\\n\\n## Reliable, one-click replacement, alert reminder\\n\\n- Reliable data migration and backfill\\n  \\n  Automatically recognize RAID disks and determine if the data migration and backfill is required to guarantee data reliability.\\n\\n- One-click disk replacement\\n  \\n  This feature is implemented by using the disk uuid.\\n\\n- Intuitive alert reminder\\n  \\n  If any exceptions occur in the process of one-click disk replacement, the system will raise an alert to remind you.\\n\\n## Join us\\n\\nIf the coming future is an era of intelligent Internet, developers will be the pioneers to that milestone, and the open source community will become the \\"metaverse\\" of developers.\\n\\nIf you have any questions about the HwameiStor cloud-native local storage system, welcome to join the community to explore this metaverse world dedicated for developers and grow together."},{"id":"1","metadata":{"permalink":"/blog/1","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-04-25-long-blog-post.md","source":"@site/blog/2022-04-25-long-blog-post.md","title":"HwameiStor Comes Online","description":"HwameiStor, an automated, highly available, cloud native, local storage system, is coming online","date":"2022-04-25T00:00:00.000Z","formattedDate":"April 25, 2022","tags":[{"label":"hello","permalink":"/blog/tags/hello"},{"label":"Hwameistor","permalink":"/blog/tags/hwameistor"}],"readingTime":2.59,"truncated":true,"authors":[{"name":"Michael Yao","title":"Technical Writer","url":"https://github.com/windsonsea","imageURL":"https://github.com/windsonsea.png","key":"Michael"}],"frontMatter":{"slug":"1","title":"HwameiStor Comes Online","authors":"Michael","tags":["hello","Hwameistor"]},"prevItem":{"title":"Reliable Helper System for HwameiStor Is Online","permalink":"/blog/2"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome"}},"content":"HwameiStor, an automated, highly available, cloud native, local storage system, is coming online\\n\\n\x3c!--truncate--\x3e\\n\\n![HwameiStor](images/HwameiStor.png)\\n\\n> News today: The local storage system provided by HwameiStor is creating a new land of \\"metaverse\\" that belongs to developers and evolves rapidly, waiting for you to join.\\n\\nDaocloud officially launched the open source project today. HwameiStor creates a local storage resource pool for centrally managing all disks such as HDD, SSD, and NVMe. It uses the CSI architecture to provide distributed services with local volumes, and provides data persistence capabilities for stateful cloud-native workloads or components.\\n\\n![System architecture](images/architect.jpg)\\n\\nIn the cloud native era, application developers can focus on the business logic itself, while the agility, scalability, and reliability required by the application runtime attribute to the infrastructure platform and O&M team. **Hwameistor is a storage system that grows in the cloud native era. **It has the advantages of high availability, automation, cost-efficiency, rapid deployment, and high performance. It can replace the expensive traditional SAN storage.\\n\\n## The local storage is smart, stable, and agile\\n\\n- Automatic operation and maintenance management\\n  \\n  Automatically discover, identify, manage, and allocate disks. Intelligently schedule applications and data based on affinity. Automatically monitor the disk status and give early warning in time.\\n\\n- Highly available data\\n  \\n  Use inter-node replicas to synchronize data for high availability. When a problem occurs, the application will be automatically scheduled to a highly available data node to guarantee the application continuity.\\n\\n- Multiple data volume types are supported\\n  \\n  Aggregate HDD, SSD, and NVMe disks to provide data service with low latency and high throughput.\\n\\n- Flexible and dynamic linear expansion\\n  \\n  A dynamic expansion is supported according to the cluster size, to flexibly meet the data persistence needs of applications.\\n\\n## Enrich scenarios and widely adapt to enterprise needs\\n\\n- Adapt to middlewares with high available architecture\\n  \\n  Kafka, Elasticsearch, Redis, and other middleware applications have high available architecture and strict requirements for IO data access. The LVM-based single-replica local data volume provided by HwameiStor can well meet their requirements.\\n\\n- Provide highly available data volumes for applications\\n  \\n  MySQL and other OLTP databases require the underlying storage to provide highly available data storage, which can quickly restore data in case of problems. At the same time, it is also required to guarantee high-performance data access. The dual-replica high available data volume provided by HwameiStor can well meet such requirements.\\n\\n- Automated operation and maintenance of traditional storage software\\n  \\n  MinIO, Ceph, and other storage software need to use the disks on a kubernetes node. These software can utilize PVC/PV to automatically use the single-replica local volume of HwameiStor through CSI drivers, quickly respond to the deployment, expansion, migration, and other requests from the business system, and realize the automatic operation and maintenance based on Kubernetes.\\n\\n## Join us\\n\\nIf the coming future is an era of intelligent Internet, developers will be the pioneers to that milestone, and the open source community will become the \\"metaverse\\" of developers.\\n\\nIf you have any questions about the HwameiStor cloud-native local storage system, welcome to join the community to explore this metaverse world dedicated for developers and grow together."},{"id":"welcome","metadata":{"permalink":"/blog/welcome","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-04-22-welcome/index.md","source":"@site/blog/2022-04-22-welcome/index.md","title":"Welcome","description":"Welcome to the Hwameistor blog space.","date":"2022-04-22T00:00:00.000Z","formattedDate":"April 22, 2022","tags":[{"label":"hello","permalink":"/blog/tags/hello"},{"label":"Hwameistor","permalink":"/blog/tags/hwameistor"}],"readingTime":0.4,"truncated":false,"authors":[{"name":"Michael Yao","title":"Technical Writer","url":"https://github.com/windsonsea","imageURL":"https://github.com/windsonsea.png","key":"Michael"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["Michael"],"tags":["hello","Hwameistor"]},"prevItem":{"title":"HwameiStor Comes Online","permalink":"/blog/1"}},"content":"Welcome to the Hwameistor blog space.\\n\\nHere you can keep up with the progress of the Hwameistor open source project and recent hot topics.\\n\\nWe also plan to include release notes for major releases, guidance articles, community-related events, and possibly some development tips, and interesting topics within the team.\\n\\nIf you are interested in contributing to this open source project and would like to join the discussion or make some guest blog posts, please contact us.\\n\\nGitHub address is: https://github.com/hwameistor"}]}')}}]);